{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• Introduction to coding Neural Networks with PyTorch \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a lot more imported libraries in this notebook, as we are going to be \n",
    "- reading in data from a file \n",
    "- and also implimenting some higher level machine learning techniques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§û Download the data\n",
    "Lets first try and download the planetary data that we need from where they are stored on google drive. Notice that the file is in a weird format (`.hdf5`) and you probably can't open it normally just by clicking on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gdown https://drive.google.com/uc?id=1X1dKqy5UR2jEp0j4RYcHBWjtYEK3o6-v -O data.hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåå What is an HDF5 File?\n",
    "\n",
    "HDF5 (Hierarchical Data Format v5) is a powerful file format designed to store and organize large amounts of data efficiently. For astrophysics, this is especially useful because it can handle **complex, multi-dimensional datasets** (like simulations or observations üå†) while maintaining high performance in reading and writing data.\n",
    "\n",
    "### Key Features:\n",
    "- **Hierarchical Structure**: Think of it as a file-system-within-a-file üóÇÔ∏è. You can organize data into groups and datasets for easy access.\n",
    "- **Efficient Storage**: Supports compression and chunking, ideal for managing massive datasets from telescopes or simulations üõ∞Ô∏è.\n",
    "- **Metadata Support**: Attach descriptive information to your data for better context and usability üè∑Ô∏è.\n",
    "\n",
    "HDF5 files are widely used in astrophysics for handling datasets like cosmic microwave background maps, galaxy catalogs, particle simulations, or in this case, exoplanet observations. üöÄ‚ú®\n",
    "\n",
    "If we open the file in python using the xarray package, then we can see that it contains 22 seperate folders, each with data in. The file gives us some information about this data at the top as well, most importantly the dimensions. We are specifically interested in the wavelength, as this will be where the raw spectra data are stored. Samples will tell us how many examples of spectra that we have, which should be around 70,000 ü™ê\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open hdf5\n",
    "path = \"data.hdf5\"\n",
    "\n",
    "ds = xr.open_dataset(path)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëÄ Let's visualise some of this data\n",
    "The below code will extract the first (`i=0`) planet from the data and plot the spectra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "# plt.plot(ds['wavelength'],ds['contributions'].sel(sample=i,species=['H2O']).values.T, label='H2O')\n",
    "\n",
    "plt.plot(ds['wavelength'],ds['spectrum'].sel(sample=i).values, \"k-\", label='observed spectrum')\n",
    "\n",
    "plt.errorbar(ds['wavelength'],ds['spectrum'].sel(sample=i).values,xerr=ds['bin_width']/2, yerr=ds['noise'].sel(sample=i).values, fmt='none', color='black', )\n",
    "\n",
    "\n",
    "plt.xlabel('wavelength')\n",
    "plt.ylabel('transit depth')\n",
    "\n",
    "axs = plt.gca()\n",
    "axs.set_xscale('log')\n",
    "axs.set_xticks([0.6, 1,1.5, 2, 2.5,3,4, 5, 6,8,])\n",
    "axs.get_xaxis().set_major_formatter(plt.ScalarFormatter())\n",
    "\n",
    "plt.title(f'sample {i}')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# print the species present in the sample\n",
    "s_ = 'log_H2O'\n",
    "print(\"#\"*70)\n",
    "print(f\" Aundance of {s_} in planetary atmosphere for planet {i} is {ds[s_].sel(sample=i).values:.4f}\")\n",
    "print(\"#\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Convert HDF5 Files to NumPy Arrays\n",
    "\n",
    "While HDF5 files are excellent for **storing and organizing large datasets**, working with them directly in Python can be less efficient for certain tasks, especially in **machine learning** workflows with frameworks like PyTorch. Here's why converting HDF5 data to **NumPy arrays** is a great idea:\n",
    "\n",
    "### ‚ö° Speed and Performance\n",
    "- NumPy arrays are highly optimized for numerical computations and are faster to manipulate in Python üèéÔ∏è.\n",
    "- Machine learning ibraries like PyTorch which we will be using natively support NumPy arrays, allowing for seamless conversion to tensors and faster data loading üîÑ.\n",
    "\n",
    "### üìö Simplicity\n",
    "- NumPy arrays are easier to slice, index, and manipulate compared to hierarchical structures in HDF5 files üß©.\n",
    "- Simplifies data preprocessing and transformation pipelines for machine learning.\n",
    "- We can also get rid of a lot of the unnessecary data in the HDF5 file that we are not interested in this time around\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['log_H2O']\n",
    "labels = ds[label_names]\n",
    "labels_np = labels.to_array().values.T\n",
    "\n",
    "labels_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra_np = ds['spectrum'].values\n",
    "spectra_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Creating the Neural Network Model \n",
    "\n",
    "This is where we will define the architecture of the model that we are going to train\n",
    "\n",
    "### üöÄ Model Structure\n",
    "\n",
    "`self.fc1 = nn.Linear(a, b)` creates a layer with `a` input features and `b` output features.\n",
    "\n",
    "So, `self.fc1 = nn.Linear(52, 512)` creates a layer with 52 input features and 512 output features.\n",
    "\n",
    "The number of input features for the first layer _must match the size of the data_ and similarly the size of the output of the last layer _must match the number of things you are trying to predict_, which in this case is only 1\n",
    "\n",
    "### üßò‚Äç‚ôÄÔ∏è Flexibility \n",
    "You can easily add more layers or modify the architecture to improve the model performance for specific use cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_A(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model_A, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(52, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è Student Task: Try different models\n",
    "Have a go training model A, B and C, and see which one you think has the better performance. If you are feeling up to it, create a model of your own and try training that as well. See if you can beat the loss performance of the models I have given you here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_B(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model_B, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(52, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_C(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model_C, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(52, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model_D, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear( # Your code here...\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu( # Your code here...\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÄ Change the model used here!\n",
    "\n",
    "Modify the line below to change which model is being used to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_A() # Modify this to select the model of your choice! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we define the loss function, which is evaluating how good your model is at predicting the amount of water in the planet atmosphere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a loss function\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Understanding The Optimiser and _Learning Rate_\n",
    "\n",
    "The **learning rate** (often denoted as `lr`) is a critical hyperparameter in machine learning algorithms. It determines the amount of change that is made to the model weights each epoch to get closer to the solution.\n",
    "\n",
    "### How It Works\n",
    "- During training, the model updates its parameters (weights) to reduce the error by following the loss function.\n",
    "- The learning rate controls how much the parameters are adjusted in each update üõ†Ô∏è.\n",
    "\n",
    "### Choosing the Right Learning Rate\n",
    "- **Too High**: The model may overshoot the optimal solution, causing the loss to oscillate or even diverge üöÄ‚ùå.\n",
    "- **Too Low**: The training process will be very slow and might get stuck in local minima, which is where the model can't keep taking smaller steps to get to a better solution, but needs to try something radically different to keep exploring.\n",
    "- **Just Right**: A properly tuned learning rate helps the model converge efficiently onto the solution üéØ.\n",
    "\n",
    "The learning rate is a key factor in determining the speed and success of your training process. A good choice can make the difference between a well-trained model and one that fails to converge! üèÜ‚ú®\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we assign the optimiser, which controls the learning rate.\n",
    "\n",
    "### ‚úçÔ∏è Student Task: Learning rate exploration\n",
    "Modify the learning rate to see the effect that this has on model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0005\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üóÇÔ∏è Understanding Batch Size in Machine Learning\n",
    "\n",
    "**Batch size** is an important hyperparameter in machine learning that defines the number of **training examples** processed together before the model updates its parameters.\n",
    "\n",
    "### How It Works\n",
    "- During training, data is divided into smaller subsets called **batches**.\n",
    "- Each batch is used to calculate the **loss** and perform a single **gradient update**.\n",
    "\n",
    "### Types of Batch Sizes\n",
    "1. **Small Batch (Mini-batch)**: \n",
    "   - Size: Usually between 32 and 512.\n",
    "   - **Pros**: Faster updates, better generalization üèÉ.\n",
    "   - **Cons**: More noise in gradients üöß.\n",
    "2. **Large Batch**: \n",
    "   - Size: Can be thousands of examples.\n",
    "   - **Pros**: More stable gradients, efficient training on larger machines üíª.\n",
    "   - **Cons**: Higher memory usage, and can potentially cause overfitting, where the model has been optimised for only a certain subset of the true data, meaning it will likely perform badly on examples that are not present within the training dataset üìà.\n",
    "3. **Full Batch**: \n",
    "   - All data is processed at once.\n",
    "   - Rarely used due to memory and speed limitations üê¢.\n",
    "\n",
    "### Choosing the Right Batch Size\n",
    "- **Small Batch Sizes**: Often better for noisy datasets and limited hardware resources üéØ.\n",
    "- **Larger Batch Sizes**: Useful for smooth convergence when sufficient computational power is available üí™.\n",
    "\n",
    "### Impact on Training\n",
    "- Smaller batches add **stochasticity** to training, which is like adding noise and randomness. It helps models generalize better, because within each batch the model is training to a small subset of the data, which is likely not representative of the full distribution. This means that the model needs to learn to cope better with data which is outside of the subset of data which it was trained on.\n",
    "- Larger batches make training more deterministic, which can lead to faster convergence but may reduce generalization.\n",
    "\n",
    "The choice of batch size is a balance between **training speed, memory constraints, and model performance**. It‚Äôs often best to experiment with different sizes to find the optimal value for your task! üèÜ‚ú®\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we assign the data loader, which controls the batch size.\n",
    "\n",
    "### ‚úçÔ∏è Student Task: Batch size exploration\n",
    "Modify the batch size to see the effect that this has on model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "train_dataset = TensorDataset(torch.Tensor(spectra_np), torch.Tensor(labels_np))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Training Begins!\n",
    "\n",
    "Here we will start to train the model over a small number of epochs. Notice how the loss decreases (hopefully!) per epoch. This means the model is learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_losses = []\n",
    "epoch_losses = []\n",
    "\n",
    "\n",
    "# train the model\n",
    "model.train()\n",
    "for epoch in range(20):\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "\n",
    "    epoch_losses.append(np.mean(batch_losses))\n",
    "    batch_losses = []\n",
    "\n",
    "    print(f'epoch {epoch}, loss {epoch_losses[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëÄ Visualising the training process\n",
    "\n",
    "Lets visualise this like we were doing in class. by creating some functions which plot the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(losses, epoch):\n",
    "\n",
    "    losses = np.array(losses)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(losses, \"k-\")\n",
    "\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title(f'training loss\\nepoch: {epoch}')\n",
    "\n",
    "    plt.yscale('log')\n",
    "\n",
    "    plt.ylim(losses.mean() - 3 * losses.std(), 4)\n",
    "\n",
    "    plt.savefig('training_loss.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(predictions, labels, epoch):\n",
    "    # make sure plot is square\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(labels, predictions, \"k.\", label='predictions')\n",
    "    plt.plot(labels, labels, \"r--\", label='Ground Truth')\n",
    "    plt.xlabel('true log H2O value')\n",
    "    plt.ylabel('predicted H2O value')\n",
    "    plt.title(f'predictions\\nepoch: {epoch}')\n",
    "\n",
    "    plt.xlim(labels.min(), labels.max())\n",
    "    plt.ylim(labels.min(), labels.max())\n",
    "\n",
    "    plt.savefig('predictions.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö¥‚Äç‚ôÄÔ∏è Training for real\n",
    "\n",
    "Lets run a lot more epochs of training, with this new visualisation.\n",
    "\n",
    "_Hint: If the loss looks like it is still decreasing after the 200 epochs I have used here, then up this number, with the caviat of course that more epochs of training will take longer to run_\n",
    "\n",
    "The model should save 2 image files, which are updated as the training goes on, which you can see in the left hand bar under the folder icon üìÅ. One will show the batch prediction against the truth, and the other will show the loss history üï∞Ô∏è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_losses = []\n",
    "epoch_losses = []\n",
    "\n",
    "\n",
    "# train the model\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "\n",
    "    epoch_losses.append(np.mean(batch_losses))\n",
    "    batch_losses = []\n",
    "\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        plot_loss(epoch_losses, epoch)\n",
    "        plot_predictions(outputs.detach().numpy(), labels.detach().numpy(), epoch)\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'epoch {epoch}, loss {epoch_losses[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üö® If you are working through this **stop here** \n",
    "## We will cover this next session in class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train into train and test\n",
    "\n",
    "tts = [0.8, 0.2]\n",
    "seed = 42\n",
    "train_spectra, test_spectra, train_labels, test_labels, train_noise, test_noise = train_test_split(spectra_np, labels_np, noise_np, test_size=tts[1], random_state=seed)\n",
    "\n",
    "# print shapes in table\n",
    "print(f\"\"\"\n",
    "train_spectra: {train_spectra.shape}\n",
    "train_labels: {train_labels.shape}\n",
    "train_noise: {train_noise.shape}\n",
    "\n",
    "test_spectra: {test_spectra.shape}\n",
    "test_labels: {test_labels.shape}\n",
    "test_noise: {test_noise.shape}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.Tensor(train_spectra), torch.Tensor(train_labels))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(losses, test_losses, epoch):\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    test_losses = np.array(test_losses)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(losses, \"k-\", label='train loss')\n",
    "    plt.plot(test_losses, \"g-\", label='test loss')\n",
    "\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title(f'training loss\\nepoch: {epoch}')\n",
    "\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.ylim(test_losses.mean() - 3 * test_losses.std(), 4)\n",
    "\n",
    "    plt.savefig('training_loss.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_losses = []\n",
    "epoch_train_losses = []\n",
    "epoch_test_losses = []\n",
    "\n",
    "# train the model\n",
    "model.train()\n",
    "for epoch in range(1000):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(torch.Tensor(test_spectra))\n",
    "        test_loss = criterion(test_outputs, torch.Tensor(test_labels))\n",
    "\n",
    "    epoch_test_losses.append(test_loss.item())\n",
    "    epoch_train_losses.append(np.mean(batch_losses))\n",
    "    batch_losses = []\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        plot_loss(epoch_train_losses,epoch_test_losses, epoch)\n",
    "        plot_predictions(outputs.detach().numpy(), labels.detach().numpy(), epoch)\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'epoch {epoch}, train loss {epoch_losses[-1]}, test loss {test_loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ariel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
