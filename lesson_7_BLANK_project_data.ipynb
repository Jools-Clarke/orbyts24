{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öõüß™ Molecule Data Aquired!\n",
    "\n",
    "In this notebook, I will show you how to load and access the data for your chosen gasses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download the data from google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gdown https://drive.google.com/uc?id=1LK65H6dtv34R_evtIHZGFPJlvCDFdWA7 -O data.hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets open the data up and check what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('data.hdf5')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the planets that we generated are not physical, this means that the spectra could not be correctly generated for at least some wavelengths.\n",
    "\n",
    "This is represented in the data as `NaN` values. `NaN` stands for `N`ot `a` `N`umber, so the data point is invalid or missing.\n",
    "\n",
    "We can convert these values into zeros now so that they don't mess with the experiment. Later on we might look at how to remove these values entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra = ds['spectra'].values\n",
    "print(spectra.shape)\n",
    "\n",
    "# Use numpy.isnan to check for NaN values\n",
    "nan_count = np.isnan(spectra).sum()\n",
    "print(f\"Number of NaN values: {nan_count}\")\n",
    "\n",
    "# Remove NaN values by replacing them with 0 \n",
    "spectra = np.nan_to_num(spectra)\n",
    "print(spectra.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Plot some Spectra\n",
    "\n",
    "Lets quickly plot some of the spectra to check that it looks as we were expecting!\n",
    "\n",
    "We start by plotting just one value..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ds['wavelength'], spectra[0], color='black') # Plot the first spectrum, the 0 here means the first, so we can change that to 1 to plot the second, or 2 for the 3rd etc\n",
    "\n",
    "def plot_extras(): # Define a little function so that we don't have to repeat this code to mkae our plots look nicer\n",
    "    plt.xscale('log')\n",
    "    plt.xticks([1, 2, 3, 5, 7], ['1¬µm', '2¬µm', '3¬µm', '5¬µm', '7¬µm'])\n",
    "    plt.xlabel('Wavelength (¬µm)')\n",
    "    plt.ylabel('Tranit Depth (%)')\n",
    "\n",
    "    plt.title('Simulated Exoplanet Transmission Spectra')\n",
    "plot_extras() # Call the function we just defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot more than one spectra though we can begin to see that they are not very similar at all. This might not be the best for machine learning purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 35): # Loop through the first 35 spectra\n",
    "    plt.plot(ds['wavelength'], spectra[i], alpha=0.5) # Plot the spectra with a different colour, and a little transparency, this is what alpha=0.5 does. alpha=1 is fully opaque, and alpha=0 is fully transparent\n",
    "\n",
    "plot_extras()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß **Normalization**\n",
    "Normalization scales the data to a smaller, consistent range‚Äîusually between 0 and 1 or -1 and 1. It ensures that spectra with different scales contribute equally to the model.\n",
    "\n",
    "### Why Normalize? \n",
    "- üèãÔ∏è‚Äç‚ôÄÔ∏è **Balances Feature Contribution**: Prevents features with larger ranges (outliers) from dominating the learning process.\n",
    "- üöÄ **Improves Convergence**: Helps algorithms converge faster.\n",
    "- üß† **Enhances Model Accuracy**: Leads to better-performing models in many cases.\n",
    "\n",
    "### Common Normalization Techniques:\n",
    "1. **Min-Max Scaling**:\n",
    "\n",
    "   $x' = \\frac{x - x_{min}}{x_{max} - x_{min}}$\n",
    "\n",
    "   Scales features to a range of 0 to 1.\n",
    "   \n",
    "2. **Z-Score Standardization**: \n",
    "   \n",
    "   $x' = \\frac{x - \\mu}{\\sigma}$\n",
    "\n",
    "   Centers the data around 0 with a standard deviation of 1.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ú® Hint: **Division by ZERO**\n",
    "\n",
    "When carrying out scaling, we have no guarantee about the values which are going to be fed through the equation. If there is a case where we think that a value might go to zero, we can add a very small value ` + np.finfo(float).eps`, which is equal to $1\\times10^{-16}$, to prevent dividing by that number from causing an error!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try and impliment some min max scaling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def min_max_normaliser(x, dim=None):\n",
    "    '''normalises the input np array to the range [0, 1]'''\n",
    "\n",
    "    x = np.array(x)\n",
    "    x_min = np.min(x, axis=dim, keepdims=True)\n",
    "    x_max = np.max(x, axis=dim, keepdims=True)\n",
    "    \n",
    "    x = x # <---- your code here...\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets plot some of the normalised spectra...\n",
    "\n",
    "What does this look like? What should this look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 35):\n",
    "    plt.plot(ds['wavelength'], min_max_normaliser(spectra, dim=1)[i], alpha=0.5)\n",
    "\n",
    "plot_extras()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load our labels, the actual values of abundance that we are training against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_header = ds['molecules'].values\n",
    "chemical_abundances = ds['log_abundance'].values\n",
    "\n",
    "print(mol_header)\n",
    "print(chemical_abundances.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want to train a model that predicts all of these gasses at once, so we just choose the one/ones that we are interested in, for example here $H_2O$ and $CH_4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "included_molecules = ['H2O', 'CH4']\n",
    "index = np.isin(mol_header, included_molecules)\n",
    "\n",
    "labels = chemical_abundances[:, index]\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Test and Train Data!\n",
    "\n",
    "While we could then train our model on all the data, because we are looking to validate the performance of the models, we hold back some of the data that the model has not seen before so that we can evaluate its performance! We will use 20\\% of the data for testing, and train on the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train into train and test - we will use the test data to evaluate the model\n",
    "\n",
    "tts = [0.8, 0.2] # 80% train, 20% test\n",
    "seed = 42 # random seed for reproducibility\n",
    "train_spectra, test_spectra, train_labels, test_labels = train_test_split(\n",
    "    spectra, # here we input the regular spectra, but we should probably use the normalised spectra\n",
    "    labels, \n",
    "    test_size=tts[1], \n",
    "    random_state=seed)\n",
    "\n",
    "# print shapes in table\n",
    "print(f\"\"\"\n",
    "train_spectra: {train_spectra.shape}\n",
    "train_labels: {train_labels.shape}\n",
    "\n",
    "test_spectra: {test_spectra.shape}\n",
    "test_labels: {test_labels.shape}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ü§ñ Model: \n",
    "\n",
    "Define your best model from previous sessions here! This is just an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_A(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model_A, self).__init__()\n",
    "\n",
    "        self.fc1 = # Your code here...\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = # Your code here...\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = # Your code here...\n",
    "lr = # Your code here...\n",
    "batch_size = # Your code here...\n",
    "\n",
    "# create a loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "# use GPU if available for faster training times\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('Using GPU')\n",
    "elif torch.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print('Using MPS')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('Using CPU')\n",
    "\n",
    "model.to(device)\n",
    "test_spectra = torch.Tensor(test_spectra).to(device)\n",
    "test_labels = torch.Tensor(test_labels).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "train_dataset = TensorDataset(torch.Tensor(train_spectra).to(device), torch.Tensor(train_labels).to(device))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_losses = []\n",
    "epoch_train_losses = []\n",
    "epoch_test_losses = []\n",
    "\n",
    "# train the model\n",
    "model.train()\n",
    "for epoch in range(50): # Keep the batch size low so we can train this quickly for today...\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(torch.Tensor(test_spectra))\n",
    "        test_loss = criterion(test_outputs, test_labels)\n",
    "\n",
    "    epoch_test_losses.append(test_loss.item())\n",
    "    epoch_train_losses.append(np.mean(batch_losses))\n",
    "    batch_losses = []\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'epoch {epoch}, train loss {epoch_train_losses[-1]}, test loss {test_loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Validation:\n",
    "\n",
    "We will use our test data that the model has never seen in order to test how accurate the model is with it's predictions. Then we can create some plots the same as last time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the test set predictions\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(test_spectra)\n",
    "\n",
    "# plot the final predictions\n",
    "for i in range(labels.shape[1]):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(test_labels.cpu().numpy()[:, i], test_outputs.cpu().numpy()[:, i],'kx', label=mol_header[i], alpha=0.5)\n",
    "\n",
    "    plt.plot([test_labels.cpu().numpy()[:, i].min(), \n",
    "            test_labels.cpu().numpy()[:, i].max()], \n",
    "            [test_labels.cpu().numpy()[:, i].min(), \n",
    "            test_labels.cpu().numpy()[:, i].max()], 'r--', lw=2)\n",
    "\n",
    "    plt.xlabel(f'True Abundance of {mol_header[index][i]}')\n",
    "    plt.ylabel(f'Predicted Abundance of {mol_header[index][i]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this look good? Is the model performing as expected? If not, consider, is the normalisation working as anticipated? Are there different ways that you could normalise the data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print a quick overview of what we have achieved here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'best test loss: {min(epoch_test_losses)}')\n",
    "print('---')\n",
    "print(f'model architecture: {model}')\n",
    "print(f'learning rate: {lr}')\n",
    "print(f'batch size: {batch_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üå± **Data Augmentation**\n",
    "Could these results be better? If we had more data...? Augmentation is the process of artificially increasing the size of your dataset by creating modified versions of existing data. \n",
    "\n",
    "### Why Augment? \n",
    "- üìà **Boosts Dataset Size**: Helps when data is limited.\n",
    "- üîÑ **Introduces Variability**: Makes the model more robust to unseen data.\n",
    "- üèÜ **Reduces Overfitting**: Prevents the model from memorizing the training data.\n",
    "\n",
    "### Examples of Augmentation in Regression:\n",
    "- **Noise Injection**: Add a small amount of random noise to input features.\n",
    "- **Synthetic Data Creation**: Generate new samples using domain knowledge or interpolation between existing samples.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we double our dataset size by creating noisy version of the data? We know how to create random values like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_spectra = np.random.rand(52)\n",
    "\n",
    "plt.plot(ds['wavelength'], random_spectra, color='black')\n",
    "plot_extras()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously this on it's own is meaningless, but is there a way to combine this with the existing data, then we could use it for training....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ariel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
