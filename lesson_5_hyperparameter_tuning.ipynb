{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Hyperparameter Optimisation: Learning Rate & Batch Size  \n",
    "\n",
    "## What is Hyperparameter Optimisation?  \n",
    "Hyperparameters are settings that define how a machine learning model learns. Unlike model parameters, which are learned from data, hyperparameters must be set before training begins. Choosing the right hyperparameters is crucial for achieving good model performance.  \n",
    "\n",
    "## Learning Rate (lr)  \n",
    "The **learning rate** determines how much the model updates its weights with each step during training:  \n",
    "\n",
    "- **Too small**: The model learns very slowly and may get stuck in a suboptimal solution.  \n",
    "- **Too large**: The model's updates are too drastic, potentially leading to unstable training and failure to converge.  \n",
    "- **Optimal**: A balance that allows the model to learn efficiently while avoiding instability.  \n",
    "\n",
    "## Batch Size  \n",
    "The **batch size** is the number of training examples used in one update (gradient descent step):  \n",
    "\n",
    "- **Small batch sizes**: More frequent updates with higher variance, which can help models generalise but may slow training.  \n",
    "- **Large batch sizes**: More stable updates and faster training, but they may lead to poorer generalisation to new data.  \n",
    "\n",
    "\n",
    "Today we will implement a grid search to find the best combination of learning rate and batch size! üöÄ  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cells below to set up the environment we will need for this week. We start by importing some libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import RegularGridInterpolator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets download the data we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gdown https://drive.google.com/uc?id=1S3xBmi4BIs0h3tdLBIy5nqq_wIsshX1J -O interpolated_rmse.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëæ The '_Model_'\n",
    "\n",
    "This week we will be using a model that I have already pretrained, which will save us some time, but it means that we load it in from a file in a slightly different way to what we are used to - don't worry too much about this as it is probably the only time you will see it in this module!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetA:\n",
    "    def __init__(self):\n",
    "        \n",
    "        filename='interpolated_rmse.npz'\n",
    "        data = np.load(filename)\n",
    "\n",
    "        # Load interpolated RMSE data\n",
    "        self.rmse_grid = data['RMSE']\n",
    "        self.lr_grid = data['lr_grid']\n",
    "        self.bs_grid = data['bs_grid']\n",
    "        \n",
    "        # Create interpolator\n",
    "        self.interpolator = RegularGridInterpolator((self.lr_grid, self.bs_grid), self.rmse_grid.T, bounds_error=False, fill_value=np.inf)\n",
    "        \n",
    "        # Store min/max ranges\n",
    "        self.lr_min, self.lr_max = np.min(self.lr_grid), np.max(self.lr_grid)\n",
    "        self.bs_min, self.bs_max = np.min(self.bs_grid), np.max(self.bs_grid)\n",
    "    \n",
    "    def train(self, learning_rate, batch_size):\n",
    "        # Check if within bounds\n",
    "        if not (self.lr_min <= learning_rate <= self.lr_max and self.bs_min <= batch_size <= self.bs_max):\n",
    "            return np.inf\n",
    "        \n",
    "        # Query interpolator\n",
    "        return self.interpolator([[learning_rate, batch_size]])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still need to define the model the same as before through!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NetA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can train and test the model using a specific `lr` (learning rate) and `batch_size`. This will give us the loss, which we are trying to minimise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "loss = model.train(lr, batch_size)\n",
    "\n",
    "print(f'Loss: {loss:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è Grid Search!\n",
    "\n",
    "Rather than guessing randomly at values for `lr` and `batch_size`, try to methodically explore the parameter space. Use the tools available in python to help you!\n",
    "\n",
    "## Why Use Grid Search for Hyperparameter Optimisation?  \n",
    "Manually selecting hyperparameters can be inefficient and ineffective. A **grid search** systematically tests different combinations of hyperparameters to find the best configuration.  \n",
    "\n",
    "### Steps to Conduct a Grid Search:  \n",
    "1. **Define the hyperparameter ranges**: Choose a set of possible values for learning rate and batch size.  \n",
    "2. **Train models for each combination**: Evaluate model performance for every (learning rate, batch size) pair.  \n",
    "3. **Compare results**: Identify which combination achieves the best validation accuracy or lowest loss.  \n",
    "4. **Select the optimal hyperparameters**: Use the best settings for final model training and evaluation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ariel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
